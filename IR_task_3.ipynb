{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1_eR4okWCuSpoBqCroylabeIYsPEoOY2t",
      "authorship_tag": "ABX9TyM/rRDevqOvjh1aD6oZjjo8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mscs24037/IR-assignment_03/blob/main/IR_task_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVaedBqBaO-4",
        "outputId": "141dee0f-207d-4043-fe0e-db51cf17bb1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Setup complete!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk pandas numpy matplotlib scikit-learn\n",
        "!pip install nltk\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import defaultdict, Counter\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# NLTK data download\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "print(\"Setup complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Document Loading\n",
        "csv_path = '/content/Document/Articles.csv'\n",
        "\n",
        "encodings = ['utf-8', 'latin-1', 'iso-8859-1', 'cp1252']\n",
        "\n",
        "df = None\n",
        "for encoding in encodings:\n",
        "    try:\n",
        "        print(f\"Trying encoding: {encoding}\")\n",
        "        df = pd.read_csv(csv_path, encoding=encoding)\n",
        "        print(f\"✓ CSV loaded successfully with {encoding}!\")\n",
        "        break\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "if df is None:\n",
        "    print(\" Could not load CSV with any encoding\")\n",
        "else:\n",
        "    print(f\"Total rows: {len(df)}\")\n",
        "    print(f\"Columns: {list(df.columns)}\")\n",
        "\n",
        "    # Find text column\n",
        "    text_col = None\n",
        "    max_length = 0\n",
        "\n",
        "    for col in df.columns:\n",
        "        try:\n",
        "            avg_len = df[col].astype(str).str.len().mean()\n",
        "            if avg_len > max_length:\n",
        "                max_length = avg_len\n",
        "                text_col = col\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    print(f\"\\n✓ Text column detected: '{text_col}'\")\n",
        "\n",
        "    # Extract documents and generate doc_ids\n",
        "    documents = df[text_col].astype(str).tolist()\n",
        "    doc_ids = [f'doc_{i}' for i in df.index]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yElcnonkYyuW",
        "outputId": "d4f4bf5a-af5c-49c5-b066-eb32d88fccd3"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trying encoding: utf-8\n",
            "Trying encoding: latin-1\n",
            "✓ CSV loaded successfully with latin-1!\n",
            "Total rows: 2692\n",
            "Columns: ['Article', 'Date', 'Heading', 'NewsType']\n",
            "\n",
            "✓ Text column detected: 'Article'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqFsp4dDVwbw",
        "outputId": "c1849e3f-39c6-44f2-ec6b-2df964a7434a"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Text Preprocessing\n",
        "class SimplePreprocessor:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.stemmer = PorterStemmer()\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        # Lowercase\n",
        "        text = text.lower()\n",
        "        # Numbers aur special characters remove\n",
        "        text = re.sub(r'[^a-z\\s]', ' ', text)\n",
        "        # Extra spaces remove\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        return text\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        \"\"\"Tokenization\"\"\"\n",
        "        return text.split()\n",
        "\n",
        "    def remove_stopwords(self, tokens):\n",
        "        \"\"\"Stopwords remove\"\"\"\n",
        "        return [token for token in tokens if token not in self.stop_words and len(token) > 2]\n",
        "\n",
        "    def stem_tokens(self, tokens):\n",
        "        \"\"\"Stemming\"\"\"\n",
        "        return [self.stemmer.stem(token) for token in tokens]\n",
        "\n",
        "    def preprocess(self, text):\n",
        "        \"\"\"Complete preprocessing\"\"\"\n",
        "        text = self.clean_text(text)\n",
        "        tokens = self.tokenize(text)\n",
        "        tokens = self.remove_stopwords(tokens)\n",
        "        tokens = self.stem_tokens(tokens)\n",
        "        return ' '.join(tokens)\n",
        "preprocessor = SimplePreprocessor()\n",
        "print(\"Processing documents...\")\n",
        "processed_docs = []\n",
        "for i, doc in enumerate(documents):\n",
        "    processed_doc = preprocessor.preprocess(doc)\n",
        "    processed_docs.append(processed_doc)\n",
        "    if (i + 1) % 100 == 0:\n",
        "        print(f\"Processed {i + 1}/{len(documents)} documents\")\n",
        "\n",
        "print(f\"Preprocessing complete! Total: {len(processed_docs)} documents\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4wUzrnMi7WP",
        "outputId": "911c2467-3e71-4670-91aa-a4028ee6fc48"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing documents...\n",
            "Processed 100/2692 documents\n",
            "Processed 200/2692 documents\n",
            "Processed 300/2692 documents\n",
            "Processed 400/2692 documents\n",
            "Processed 500/2692 documents\n",
            "Processed 600/2692 documents\n",
            "Processed 700/2692 documents\n",
            "Processed 800/2692 documents\n",
            "Processed 900/2692 documents\n",
            "Processed 1000/2692 documents\n",
            "Processed 1100/2692 documents\n",
            "Processed 1200/2692 documents\n",
            "Processed 1300/2692 documents\n",
            "Processed 1400/2692 documents\n",
            "Processed 1500/2692 documents\n",
            "Processed 1600/2692 documents\n",
            "Processed 1700/2692 documents\n",
            "Processed 1800/2692 documents\n",
            "Processed 1900/2692 documents\n",
            "Processed 2000/2692 documents\n",
            "Processed 2100/2692 documents\n",
            "Processed 2200/2692 documents\n",
            "Processed 2300/2692 documents\n",
            "Processed 2400/2692 documents\n",
            "Processed 2500/2692 documents\n",
            "Processed 2600/2692 documents\n",
            "Preprocessing complete! Total: 2692 documents\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " #TF-IDF Retrieval System\n",
        "class TFIDFRetriever:\n",
        "    def __init__(self, documents, doc_ids):\n",
        "        self.documents = documents\n",
        "        self.doc_ids = doc_ids\n",
        "\n",
        "        self.vectorizer = TfidfVectorizer(\n",
        "            max_features=5000,  # Top 5000 features\n",
        "            min_df=2,           # Minimum should be in 2 documents\n",
        "            max_df=0.8,         # should be availabe more than 80%\n",
        "            ngram_range=(1, 2)  # Unigrams and bigrams\n",
        "        )\n",
        "        #make document vector\n",
        "        self.doc_vectors = self.vectorizer.fit_transform(documents)\n",
        "        print(f\"Index built! Shape: {self.doc_vectors.shape}\")\n",
        "        print(f\"Vocabulary size: {len(self.vectorizer.vocabulary_)}\")\n",
        "\n",
        "    def search(self, query, top_k=10):\n",
        "\n",
        "        # make Query vector\n",
        "        query_vector = self.vectorizer.transform([query])\n",
        "\n",
        "        # calculate Cosine similarity\n",
        "        similarities = cosine_similarity(query_vector, self.doc_vectors)[0]\n",
        "\n",
        "        # find Top-k indices\n",
        "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
        "\n",
        "        # Results prepare\n",
        "        results = []\n",
        "        for rank, idx in enumerate(top_indices, 1):\n",
        "            if similarities[idx] > 0:  # Only non-zero scores\n",
        "                results.append({\n",
        "                    'rank': rank,\n",
        "                    'doc_id': self.doc_ids[idx],\n",
        "                    'score': float(similarities[idx])\n",
        "                })\n",
        "\n",
        "        return results\n",
        "\n",
        "    def get_vocabulary_stats(self):\n",
        "        \"\"\"Vocabulary statistics\"\"\"\n",
        "        vocab = self.vectorizer.vocabulary_\n",
        "        feature_names = self.vectorizer.get_feature_names_out()\n",
        "\n",
        "        print(f\"\\nVocabulary Statistics:\")\n",
        "        print(f\"Total terms: {len(vocab)}\")\n",
        "        print(f\"Sample terms: {list(feature_names[:10])}\")\n",
        "\n",
        "    # Step 2: Create doc_ids\n",
        "    print(\"\\nCreating document IDs...\")\n",
        "    doc_ids = [f\"doc_{i+1}\" for i in range(len(processed_docs))]\n",
        "    print(f\"Created {len(doc_ids)} document IDs\")\n",
        "\n",
        "    # Step 3: Now create the retriever\n",
        "    print(\"\\nInitializing TF-IDF retriever...\")\n",
        "    retriever = TFIDFRetriever(processed_docs, doc_ids)\n",
        "    retriever.get_vocabulary_stats()\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7bzAG-Lb7ec",
        "outputId": "22330a77-ac70-4afb-da3a-32c815efb815"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Creating document IDs...\n",
            "Created 2692 document IDs\n",
            "\n",
            "Initializing TF-IDF retriever...\n",
            "Index built! Shape: (2692, 5000)\n",
            "Vocabulary size: 5000\n",
            "\n",
            "Vocabulary Statistics:\n",
            "Total terms: 5000\n",
            "Sample terms: ['aaron', 'aaron finch', 'abandon', 'abbott', 'abdul', 'abdullah', 'abil', 'abl', 'abroad', 'absenc']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Search Function\n",
        "def search_documents(query, top_k=10, show_preview=True):\n",
        "\n",
        "    # Query preprocess\n",
        "    processed_query = preprocessor.preprocess(query)\n",
        "\n",
        "    # Search\n",
        "    start_time = time.time()\n",
        "    results = retriever.search(processed_query, top_k)\n",
        "    end_time = time.time()\n",
        "\n",
        "    print(f\"Search completed in {end_time - start_time:.4f} seconds\")\n",
        "    print(f\"Found {len(results)} relevant documents\\n\")\n",
        "\n",
        "    if not results:\n",
        "        print(\"No relevant documents found!\")\n",
        "        return results\n",
        "\n",
        "    # Results display\n",
        "    for result in results:\n",
        "        print(f\"Rank {result['rank']}: {result['doc_id']}\")\n",
        "        print(f\"Relevance Score: {result['score']:.4f}\")\n",
        "\n",
        "results1 = search_documents(\"honk kong\", top_k=5, show_preview=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SA_SoKo1jgHZ",
        "outputId": "8f8e03ed-29da-4fd6-fb69-2a374f3bc5bc"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search completed in 0.0128 seconds\n",
            "Found 5 relevant documents\n",
            "\n",
            "Rank 1: doc_12\n",
            "Relevance Score: 0.3552\n",
            "Rank 2: doc_3\n",
            "Relevance Score: 0.3470\n",
            "Rank 3: doc_388\n",
            "Relevance Score: 0.3426\n",
            "Rank 4: doc_2460\n",
            "Relevance Score: 0.3224\n",
            "Rank 5: doc_68\n",
            "Relevance Score: 0.3203\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EVALUATION\n",
        "class SimpleEvaluator:\n",
        "    def __init__(self, retriever, preprocessor):\n",
        "        self.retriever = retriever\n",
        "        self.preprocessor = preprocessor\n",
        "\n",
        "    def precision_at_k(self, results, relevant_docs, k):\n",
        "        top_k = results[:k]\n",
        "        relevant_retrieved = sum(1 for r in top_k if r['doc_id'] in relevant_docs)\n",
        "        return relevant_retrieved / k if k > 0 else 0\n",
        "\n",
        "    def recall_at_k(self, results, relevant_docs, k):\n",
        "        top_k = results[:k]\n",
        "        relevant_retrieved = sum(1 for r in top_k if r['doc_id'] in relevant_docs)\n",
        "        total_relevant = len(relevant_docs)\n",
        "        return relevant_retrieved / total_relevant if total_relevant > 0 else 0\n",
        "\n",
        "    def mean_reciprocal_rank(self, results, relevant_docs):\n",
        "        for rank, result in enumerate(results, 1):\n",
        "            if result['doc_id'] in relevant_docs:\n",
        "                return 1.0 / rank\n",
        "        return 0.0\n",
        "\n",
        "# Initialize evaluator\n",
        "evaluator = SimpleEvaluator(retriever, preprocessor)\n",
        "\n",
        "# Test query\n",
        "query = 'HONG KONG'\n",
        "relevant = ['doc_5']\n",
        "\n",
        "# Process query\n",
        "processed_query = preprocessor.preprocess(query)\n",
        "results = retriever.search(processed_query, top_k=10)\n",
        "\n",
        "# Calculate metrics\n",
        "p_at_5 = evaluator.precision_at_k(results, relevant, 5)\n",
        "r_at_5 = evaluator.recall_at_k(results, relevant, 5)\n",
        "mrr = evaluator.mean_reciprocal_rank(results, relevant)\n",
        "\n",
        "# Print results\n",
        "print(f\"\\nQuery: {query}\")\n",
        "print(f\"Relevant docs: {relevant}\")\n",
        "print(f\"\\nTop 10 Results:\")\n",
        "for r in results:\n",
        "    marker = \"✓\" if r['doc_id'] in relevant else \" \"\n",
        "    print(f\"  {marker} Rank {r['rank']}: {r['doc_id']} (Score: {r['score']:.4f})\")\n",
        "\n",
        "print(f\"\\nMetrics:\")\n",
        "print(f\"  Precision@5: {p_at_5:.3f}\")\n",
        "print(f\"  Recall@5: {r_at_5:.3f}\")\n",
        "print(f\"  MRR: {mrr:.3f}\")"
      ],
      "metadata": {
        "id": "_j0xWWrDrR6Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4168098e-88a7-4b93-fc44-a1b313d3a43b"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Query: HONG KONG\n",
            "Relevant docs: ['doc_5']\n",
            "\n",
            "Top 10 Results:\n",
            "    Rank 1: doc_12 (Score: 0.6156)\n",
            "    Rank 2: doc_3 (Score: 0.6015)\n",
            "    Rank 3: doc_388 (Score: 0.5939)\n",
            "    Rank 4: doc_2460 (Score: 0.5587)\n",
            "    Rank 5: doc_68 (Score: 0.5552)\n",
            "    Rank 6: doc_1408 (Score: 0.5458)\n",
            "    Rank 7: doc_113 (Score: 0.4813)\n",
            "    Rank 8: doc_1406 (Score: 0.4670)\n",
            "    Rank 9: doc_567 (Score: 0.4388)\n",
            "    Rank 10: doc_2590 (Score: 0.4348)\n",
            "\n",
            "Metrics:\n",
            "  Precision@5: 0.000\n",
            "  Recall@5: 0.000\n",
            "  MRR: 0.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#System Statistics\n",
        "def display_system_statistics():\n",
        "\n",
        "    print(f\"\\n Document Statistics:\")\n",
        "    print(f\"  Total documents: {len(documents)}\")\n",
        "\n",
        "    doc_lengths = [len(doc) for doc in documents]\n",
        "    print(f\"  Avg characters per doc: {np.mean(doc_lengths):.0f}\")\n",
        "    print(f\"  Min characters: {np.min(doc_lengths)}\")\n",
        "    print(f\"  Max characters: {np.max(doc_lengths)}\")\n",
        "\n",
        "    # Processed documents\n",
        "    processed_lengths = [len(doc.split()) for doc in processed_docs]\n",
        "    print(f\"\\n  After preprocessing:\")\n",
        "    print(f\"  Avg tokens per doc: {np.mean(processed_lengths):.0f}\")\n",
        "    print(f\"  Min tokens: {np.min(processed_lengths)}\")\n",
        "    print(f\"  Max tokens: {np.max(processed_lengths)}\")\n",
        "\n",
        "    # TF-IDF statistics\n",
        "    print(f\"\\n TF-IDF Index Statistics:\")\n",
        "    print(f\"  Vocabulary size: {len(retriever.vectorizer.vocabulary_)}\")\n",
        "    print(f\"  Matrix shape: {retriever.doc_vectors.shape}\")\n",
        "    print(f\"  Matrix density: {retriever.doc_vectors.nnz / (retriever.doc_vectors.shape[0] * retriever.doc_vectors.shape[1]):.4f}\")\n",
        "\n",
        "display_system_statistics()"
      ],
      "metadata": {
        "id": "gvBiipJqsm18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27324dcc-92a4-4e79-90ce-7078c8e91504"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Document Statistics:\n",
            "  Total documents: 2692\n",
            "  Avg characters per doc: 1810\n",
            "  Min characters: 216\n",
            "  Max characters: 19199\n",
            "\n",
            "  After preprocessing:\n",
            "  Avg tokens per doc: 178\n",
            "  Min tokens: 23\n",
            "  Max tokens: 1784\n",
            "\n",
            " TF-IDF Index Statistics:\n",
            "  Vocabulary size: 5000\n",
            "  Matrix shape: (2692, 5000)\n",
            "  Matrix density: 0.0255\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Complete Test Run\n",
        "def search_documents(query, top_k=5, show_preview=False):\n",
        "\n",
        "    if 'retriever' not in globals():\n",
        "        print(\"ERROR: TFIDFRetriever not initialized. Run setup first.\")\n",
        "        return []\n",
        "    results = retriever.search(query, top_k=top_k)\n",
        "\n",
        "# Multiple test queries\n",
        "test_queries = [\n",
        "    \"artificial intelligence\",\n",
        "    \"computer networks\"\n",
        "]\n",
        "\n",
        "print(\"\\nRunning test queries...\\n\")\n",
        "\n",
        "for i, query in enumerate(test_queries, 1):\n",
        "    print(f\"\\nTest {i}/{len(test_queries)}\")\n",
        "    print(f\"Query: '{query}'\")\n",
        "\n",
        "    results = search_documents(query, top_k=3, show_preview=False)\n",
        "\n",
        "    if results:\n",
        "        print(f\"Found {len(results)} results:\")\n",
        "        for result in results:\n",
        "            print(f\"  Rank {result['rank']}: {result['doc_id']} (Score: {result['score']:.4f})\")\n",
        "    else:\n",
        "        print(\"  No results found\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LaztmmP3gcBU",
        "outputId": "e1090a5e-3ec1-465b-9e5b-319857e50f96"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running test queries...\n",
            "\n",
            "\n",
            "Test 1/2\n",
            "Query: 'artificial intelligence'\n",
            "  No results found\n",
            "\n",
            "Test 2/2\n",
            "Query: 'computer networks'\n",
            "  No results found\n"
          ]
        }
      ]
    }
  ]
}